{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for column in df.columns:\n",
    "        max_value = df[column].max()\n",
    "        min_value = df[column].min()\n",
    "        if(max_value-min_value!=0):\n",
    "            result[column] = (df[column] - min_value) / (max_value - min_value) \n",
    "        else:\n",
    "            result[column]=df[column]\n",
    "    return(result)  \n",
    "normalised_train=normalize(pd.read_csv(\"./medical_training_data.csv\"))\n",
    "normalised_test=normalize(pd.read_csv(\"./medical_testing_data.csv\"))\n",
    "\n",
    "\n",
    "normalised_train_data=np.transpose(normalised_train.iloc[:,1:16].to_numpy())\n",
    "\n",
    "\n",
    "train_result=normalised_train.iloc[:,16].to_numpy()\n",
    "train_result=np.transpose(train_result[:,np.newaxis])\n",
    "\n",
    "normalised_test_data=np.transpose(normalised_test.iloc[:,1:16].to_numpy())\n",
    "\n",
    "test_result=normalised_test.iloc[:,16].to_numpy()\n",
    "test_result=np.transpose(test_result[:,np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in epoch 0 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 1 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 2 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 3 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 4 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 5 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 6 is 15.643224699828473 percentage\n",
      "Accuracy in epoch 7 is 75.64322469982847 percentage\n",
      "Accuracy in epoch 8 is 83.9794168096055 percentage\n",
      "Accuracy in epoch 9 is 84.28816466552315 percentage\n",
      "Accuracy in epoch 10 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 11 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 12 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 13 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 14 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 15 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 16 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 17 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 18 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 19 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 20 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 21 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 22 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 23 is 84.35677530017152 percentage\n",
      "Accuracy in epoch 24 is 84.35677530017152 percentage\n",
      "accuracy percentage on your  testing data is 86.23481781376519 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ann:\n",
    "    def __init__(self,train,train_result,test,test_result,epoches,alpha,hidden_layers,l):  # l to input the activation functions between each layer  #  hidden layers is a list passed in which number of features for each hidden layer \n",
    "        \n",
    "        self.train=train\n",
    "        self.train_result=train_result\n",
    "        self.test=test \n",
    "        self.test_result=test_result\n",
    "        self.epoches=epoches\n",
    "        self.alpha=alpha\n",
    "        self.l=l          # activation function between each rows\n",
    "\n",
    "        self.input_features=np.shape(train)[0] # along the rows \n",
    "        self.output_features = np.shape(train_result)[0]\n",
    "        self.m=np.shape(train)[1]      # along the columns in a row    \n",
    "\n",
    "        self.layers=[self.input_features]+hidden_layers+[self.output_features]   # contains number of features in each layer including start , hidden and end layers\n",
    "\n",
    "\n",
    "        weights=[]     \n",
    "        bias=[]\n",
    "         \n",
    "        \n",
    "        error_wrt_z=[]\n",
    "        error_wrt_w=[]\n",
    "        error_wrt_b=[]\n",
    "\n",
    "        for i in range(len(self.layers)-1): \n",
    "            a=np.random.rand (self.layers[i+1],self.layers[i])\n",
    "            weights.append(a)\n",
    "\n",
    "            b=np.random.rand(self.layers[i+1],1)          # will use broad casting                                              \n",
    "            bias.append(b) \n",
    "\n",
    "            error_wrt_w.append(i)    # how many dw and db will be there , later i's will be updated with dw and db matrices \n",
    "\n",
    "            error_wrt_b.append(i)      # there will be number of (layers - 1) matrices for weights,bias,error wrt w,b,z in each forward propagation   \n",
    "\n",
    "            error_wrt_z.append(i)  \n",
    "        \n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "\n",
    "       \n",
    "        self.error_wrt_z= error_wrt_z\n",
    "        self.error_wrt_w= error_wrt_w\n",
    "        self.error_wrt_b= error_wrt_b\n",
    "\n",
    "        self.activations=[]\n",
    "        self.z=[]\n",
    "\n",
    "        self.function_derivitive=[self.reluDerivative,self.sigmoid_derivative]  # in middle layers put relu or sigmoid only you cannnot put softmax in middle as you dont know d softmax(z) / d(z)\n",
    "        self.function=[self.relu,self.sigmoid,self.softmax]  \n",
    "\n",
    "    \n",
    "\n",
    "    def relu(self,x):\n",
    "        return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "    def sigmoid(self , x):\n",
    "        y=1/(1+np.exp(-x) )                       \n",
    "        return y\n",
    "        \n",
    "    def sigmoid_derivative(self, x):        #  d sigma(z) / d(z) = sigma(z)*(1-sigma(z)) \n",
    "        return np.multiply(x,(1-x))         # in this we are finding for all the a's of the matrix \n",
    "\n",
    "    def reluDerivative(self,x):\n",
    "        y=np.copy(x)\n",
    "        y[y<=0] = 0\n",
    "        y[y>0] = 1\n",
    "        return y     \n",
    "\n",
    "    def softmax (self,x):\n",
    "        e=np.exp(x)\n",
    "        return e/(np.sum(e,axis=0))  \n",
    "\n",
    "\n",
    "    def forward_propogate(self,data): # we are using forward prop on training as well as testing data so \n",
    "        self.activations.clear()   #  with each epoch our w will be updated as dw db dz will be updated so we will be getting new activations as well as z  after each eppoch\n",
    "        self.z.clear()\n",
    "        self.z.append(data)\n",
    "        self.activations.append(data)        ### note z and a will contain all the layers from  1st given layer at indeex 0 to the last output predicted layer \n",
    "      \n",
    "        for i in range (len(self.layers)-1):\n",
    "            self.z.append(np.add (np.dot(self.weights[i],self.activations[i]) ,self.bias[i]))\n",
    "        \n",
    "           # self.z.append(np.add( np.dot( self.weights[i],self.activations[i] ) ,np.repeat(self.bias[i],self.m,axis=1)) )    \n",
    "            self.activations.append(self.function[self.l[i]](self.z[i+1]  ) )  \n",
    "\n",
    "\n",
    "\n",
    "    def backward_propogation(self):  # we dont know d softmax(z) / d (z) so we are not using dz formula for last layer \n",
    "        self.error_wrt_z[len(self.layers)-2]=np.subtract(self.activations[len(self.layers)-1] , self.train_result )\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            self.error_wrt_w[i] = 1/self.m*(np.dot((self.error_wrt_z[i]),np.transpose(self.activations[i])))  \n",
    "            \n",
    "            self.error_wrt_b[i]=1/self.m *(np.sum(self.error_wrt_z[i] , axis=1 , keepdims=True))\n",
    "\n",
    "            if i>=1:\n",
    "                    self.error_wrt_z[i-1]=np.multiply (  np.dot ( np.transpose (self.weights[i]) , self.error_wrt_z[i] )  , self.function_derivitive[ self.l[i-1] ] ( self.function[ self.l[i-1] ]   (self.z[i]) ) )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def training(self):\n",
    "        for i in range(self.epoches):\n",
    "            self.forward_propogate(data=self.train)\n",
    "           \n",
    "            final_prediction=self.predict()\n",
    "            accuracy_percentage=self.accuracy(final_prediction,actual_result=self.train_result)\n",
    "            print(\"Accuracy in epoch {} is {} percentage\".format(i,accuracy_percentage))\n",
    "            self.backward_propogation()\n",
    "            for j in range(len (self.layers)-1):\n",
    "                self.weights[j]=np.subtract(self.weights[j],self.alpha*self.error_wrt_w[j])\n",
    "                \n",
    "                self.bias[j]=np.subtract(self.bias[j],self.alpha*self.error_wrt_b[j])\n",
    "    \n",
    "    def predict(self):  \n",
    "             # output is single (binary classigication using  sigmoid)   # [1 0 1 1 0 0 ] like this\n",
    "       final_prediction=0.5<=self.activations[len(self.layers)-1] \n",
    "       return(final_prediction.astype(int))    \n",
    "           \n",
    "       \n",
    "   \n",
    "    def accuracy(self,final_prediction,actual_result):\n",
    "        accuracy_per = (np.sum(actual_result ==final_prediction) / (actual_result).shape[1] ) * 100\n",
    "        return accuracy_per\n",
    "    # after training when we are having updated w  and b we predict result of testing set \n",
    "    \n",
    "    def Testing(self):\n",
    "        self.forward_propogate(data=self.test)  # we will get all activations layers including the last predicted layer \n",
    "        binary_prediction=self.predict()\n",
    "        accuracy_percentage=self.accuracy(binary_prediction,actual_result=self.test_result)\n",
    "        print(\"accuracy percentage on your  testing data is {} \".format(accuracy_percentage))\n",
    "\n",
    "\n",
    "a=ann(train=normalised_train_data,train_result=train_result,test=normalised_test_data,test_result=test_result,epoches=250,alpha=0.001,hidden_layers=[8,4],l=[0,0,1])\n",
    "\n",
    "\n",
    "a.training()\n",
    "a.Testing()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae54068e622f61d47b992eee920515c98b314ebd4be04c0239c7d903a657733"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
