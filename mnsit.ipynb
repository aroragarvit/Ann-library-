{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel0      0\n",
      "pixel1      0\n",
      "pixel2      0\n",
      "pixel3      0\n",
      "pixel4      0\n",
      "           ..\n",
      "pixel779    0\n",
      "pixel780    0\n",
      "pixel781    0\n",
      "pixel782    0\n",
      "pixel783    0\n",
      "Length: 784, dtype: int64\n",
      "Shape of training data (784, 42000) \n",
      "Shape of encoded labels are (10, 42000)\n",
      "Shape of training labels (1, 42000) \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "from keras.utils import to_categorical\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for column in df.columns:\n",
    "        max_value = df[column].max()\n",
    "        min_value = df[column].min()\n",
    "        if(max_value-min_value!=0):\n",
    "            result[column] = (df[column] - min_value) / (max_value - min_value) \n",
    "        else:\n",
    "            result[column]=df[column]\n",
    "    return(result)  \n",
    "\n",
    "data=pd.read_csv(\"train.csv\")\n",
    "\n",
    "training_data=normalize(data.iloc[:,1:785])\n",
    "print(training_data.isna().sum())\n",
    "\n",
    "training_data=np.transpose(training_data.to_numpy())  # our data for training\n",
    "print(\"Shape of training data {} \".format(training_data.shape))\n",
    "\n",
    "training_label=data[\"label\"].to_numpy()              \n",
    "\n",
    "encoded_result = np.transpose(to_categorical(training_label))\n",
    "print(\"Shape of encoded labels are {}\".format(encoded_result.shape))\n",
    "\n",
    "training_label=np.transpose(training_label[:,np.newaxis])\n",
    "print(\"Shape of training labels {} \".format(training_label.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ann:\n",
    "    def __init__(self,train,train_result,test,test_result,epoches,alpha,hidden_layers,l):  # l to input the activation functions between each layer  #  hidden layers is a list passed in which number of features for each hidden layer \n",
    "        \n",
    "        self.train=train\n",
    "        self.train_result=train_result\n",
    "        self.test=test \n",
    "        self.test_result=test_result\n",
    "        self.epoches=epoches\n",
    "        self.alpha=alpha\n",
    "        self.l=l          # activation function between each rows\n",
    "\n",
    "        self.input_features=np.shape(train)[0] # along the rows \n",
    "        self.output_features = np.shape(train_result)[0]\n",
    "        self.m=np.shape(train)[1]      # along the columns in a row    \n",
    "\n",
    "        self.layers=[self.input_features]+hidden_layers+[self.output_features]   # contains number of features in each layer including start , hidden and end layers\n",
    "\n",
    "\n",
    "        weights=[]     \n",
    "        bias=[]\n",
    "         \n",
    "        \n",
    "        error_wrt_z=[]\n",
    "        error_wrt_w=[]\n",
    "        error_wrt_b=[]\n",
    "\n",
    "        for i in range(len(self.layers)-1): \n",
    "            a=np.random.rand (self.layers[i+1],self.layers[i])\n",
    "            weights.append(a)\n",
    "\n",
    "            b=np.random.rand(self.layers[i+1],1)          # will use broad casting                                              \n",
    "            bias.append(b) \n",
    "\n",
    "            error_wrt_w.append(i)    # how many dw and db will be there , later i's will be updated with dw and db matrices \n",
    "\n",
    "            error_wrt_b.append(i)      # there will be number of (layers - 1) matrices for weights,bias,error wrt w,b,z in each forward propagation   \n",
    "\n",
    "            error_wrt_z.append(i)  \n",
    "        \n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "\n",
    "       \n",
    "        self.error_wrt_z= error_wrt_z\n",
    "        self.error_wrt_w= error_wrt_w\n",
    "        self.error_wrt_b= error_wrt_b\n",
    "\n",
    "        self.activations=[]\n",
    "        self.z=[]\n",
    "\n",
    "        self.function_derivitive=[self.reluDerivative,self.sigmoid_derivative]  # in middle layers put relu or sigmoid only you cannnot put softmax in middle as you dont know d softmax(z) / d(z)\n",
    "        self.function=[self.relu,self.sigmoid,self.softmax]  \n",
    "\n",
    "    \n",
    "\n",
    "    def relu(self,x):\n",
    "        return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "    def sigmoid(self , x):\n",
    "        y=1/(1+np.exp(-x) )                       \n",
    "        return y\n",
    "        \n",
    "    def sigmoid_derivative(self, x):        #  d sigma(z) / d(z) = sigma(z)*(1-sigma(z)) \n",
    "        return np.multiply(x,(1-x))         # in this we are finding for all the a's of the matrix \n",
    "\n",
    "    def reluDerivative(self,x):\n",
    "        y=np.copy(x)\n",
    "        y[y<=0] = 0\n",
    "        y[y>0] = 1\n",
    "        return y     \n",
    "\n",
    "    def softmax (self,x):\n",
    "        e=np.exp(x)\n",
    "        return e/(np.sum(e,axis=0))  \n",
    "\n",
    "\n",
    "    def forward_propogate(self,data): # we are using forward prop on training as well as testing data so \n",
    "        self.activations.clear()   #  with each epoch our w will be updated as dw db dz will be updated so we will be getting new activations as well as z  after each eppoch\n",
    "        self.z.clear()\n",
    "        self.z.append(data)\n",
    "        self.activations.append(data)        ### note z and a will contain all the layers from  1st given layer at indeex 0 to the last output predicted layer \n",
    "      \n",
    "        for i in range (len(self.layers)-1):\n",
    "            self.z.append(np.add (np.dot(self.weights[i],self.activations[i]) ,self.bias[i]))\n",
    "        \n",
    "           # self.z.append(np.add( np.dot( self.weights[i],self.activations[i] ) ,np.repeat(self.bias[i],self.m,axis=1)) )    \n",
    "            self.activations.append(self.function[self.l[i]](self.z[i+1]  ) )  \n",
    "\n",
    "\n",
    "\n",
    "    def backward_propogation(self):  # we dont know d softmax(z) / d (z) so we are not using dz formula for last layer \n",
    "        self.error_wrt_z[len(self.layers)-2]=np.subtract(self.activations[len(self.layers)-1] , self.train_result )\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            self.error_wrt_w[i] = 1/self.m*(np.dot((self.error_wrt_z[i]),np.transpose(self.activations[i])))  \n",
    "            \n",
    "            self.error_wrt_b[i]=1/self.m *(np.sum(self.error_wrt_z[i] , axis=1 , keepdims=True))\n",
    "\n",
    "            if i>=1:\n",
    "                    self.error_wrt_z[i-1]=np.multiply (  np.dot ( np.transpose (self.weights[i]) , self.error_wrt_z[i] )  , self.function_derivitive[ self.l[i-1] ] ( self.function[ self.l[i-1] ]   (self.z[i]) ) )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def training(self):\n",
    "        for i in range(self.epoches):\n",
    "            self.forward_propogate(data=self.train)\n",
    "           \n",
    "            final_prediction=self.predict()\n",
    "            accuracy_percentage=self.accuracy(final_prediction,actual_result=self.train_result)\n",
    "            print(\"Accuracy in epoch {} is {} percentage\".format(i,accuracy_percentage))\n",
    "            self.backward_propogation()\n",
    "            for j in range(len (self.layers)-1):\n",
    "                self.weights[j]=np.subtract(self.weights[j],self.alpha*self.error_wrt_w[j])\n",
    "                \n",
    "                self.bias[j]=np.subtract(self.bias[j],self.alpha*self.error_wrt_b[j])\n",
    "    \n",
    "    def predict(self):  \n",
    "             # output is single (binary classigication using  sigmoid)   # [1 0 1 1 0 0 ] like this\n",
    "       final_prediction=0.5<=self.activations[len(self.layers)-1] \n",
    "       return(final_prediction.astype(int))    \n",
    "           \n",
    "       \n",
    "   \n",
    "    def accuracy(self,final_prediction,actual_result):\n",
    "        accuracy_per = (np.sum(actual_result ==final_prediction) / (actual_result).shape[1] ) * 100\n",
    "        return accuracy_per\n",
    "    # after training when we are having updated w  and b we predict result of testing set \n",
    "    \n",
    "    #def Testing(self):\n",
    "    #    self.forward_propogate(data=self.test)  # we will get all activations layers including the last predicted layer \n",
    "    #    binary_prediction=self.predict()\n",
    "    #    accuracy_percentage=self.accuracy(binary_prediction,actual_result=self.test_result)\n",
    "    #    print(\"accuracy percentage on your  testing data is {} \".format(accuracy_percentage))\n",
    "\n",
    "\n",
    "a=ann(train=training_data,train_result=encoded_result,epoches=250,alpha=0.003,hidden_layers=[100,25],l=[0,0,2])\n",
    "\n",
    "\n",
    "a.training()\n",
    "a.Testing()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae54068e622f61d47b992eee920515c98b314ebd4be04c0239c7d903a657733"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
